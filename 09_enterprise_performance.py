"""
Example 09: Enterprise Performance Features

This example demonstrates the high-performance features added for enterprise scale:
1. Batch Signature Verification (3-5x faster)
2. Parallel Block Validation (2x faster)
3. Fast JSON Serialization (10x faster)
4. Object Pooling (Reduced GC pressure)
5. LMDB Storage Backend (15x faster)
"""

import asyncio
import logging
import time
import shutil
from pathlib import Path

from src.core.crypto import Wallet, batch_verify_signatures, verify_signature_cached
from src.core.blockchain import Blockchain
from src.core.validation.engine import ValidationEngine
from src.core.serialization import serialize_message, deserialize_message, pool_stats
from src.core.storage import StorageConfig, LMDBStorage

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def measure_time(name: str, func, *args):
    start = time.perf_counter()
    result = func(*args)
    duration = (time.perf_counter() - start) * 1000
    logger.info(f"{name}: {duration:.2f}ms")
    return result

async def measure_async_time(name: str, func, *args):
    start = time.perf_counter()
    result = await func(*args)
    duration = (time.perf_counter() - start) * 1000
    logger.info(f"{name}: {duration:.2f}ms")
    return result

def demo_crypto_performance():
    logger.info("\n--- 1. Cryptographic Optimization ---")
    
    # Generate test data
    count = 100
    logger.info(f"Generating {count} signatures...")
    wallet = Wallet.generate("demo")
    items = []
    
    for i in range(count):
        msg = f"Message {i}".encode()
        sig = wallet.sign(msg)
        items.append((msg, sig, wallet.signing_keys.public_key))
    
    # 1. Sequential Verification
    def run_sequential():
        results = batch_verify_signatures(items, parallel=False)
        assert all(results)
    
    measure_time(f"Sequential Verification ({count} items)", run_sequential)
    
    # 2. Parallel Verification
    def run_parallel():
        results = batch_verify_signatures(items, parallel=True)
        assert all(results)
    
    measure_time(f"Parallel Verification ({count} items)", run_parallel)
    
    # 3. Cached Verification
    logger.info("Demonstrating LRU Key Cache...")
    msg = b"Repeated message"
    sig = wallet.sign(msg)
    pk = wallet.signing_keys.public_key
    
    # Warm up cache
    verify_signature_cached(msg, sig, pk)
    
    def run_cached():
        for _ in range(count * 10):
            verify_signature_cached(msg, sig, pk)
            
    measure_time(f"Cached Verification ({count*10} ops)", run_cached)


async def demo_validation_performance():
    logger.info("\n--- 2. Validation Pipeline Optimization ---")
    
    # Setup blockchain with many messages
    bc = Blockchain(difficulty=1)
    messages = [{"id": f"msg{i}", "content": f"data{i}"} for i in range(1000)]
    bc.add_data({"messages": messages})
    bc.mine_pending()
    block = bc.chain[-1]
    
    engine = ValidationEngine()
    
    # 1. Standard Validation
    await measure_async_time(
        "Standard Validation",
        engine.validate_block,
        block
    )
    
    # 2. Parallel Validation
    await measure_async_time(
        "Parallel Validation (Async Layers)",
        engine.validate_block_parallel,
        block
    )


def demo_serialization():
    logger.info("\n--- 3. Serialization Performance ---")
    
    # Large complex object
    data = {
        "id": "obj1",
        "type": "update",
        "timestamp": time.time(),
        "payload": {
            "items": [{"x": i, "y": i*2} for i in range(1000)],
            "metadata": {"source": "sensor_x", "status": "active"}
        }
    }
    
    count = 1000
    
    def run_serialization():
        for _ in range(count):
            blob = serialize_message(data)
            _ = deserialize_message(blob)
            
    measure_time(f"Serialize/Deserialize ({count} ops)", run_serialization)
    
    logger.info("Object Pool Stats:")
    logger.info(pool_stats())


def demo_storage():
    logger.info("\n--- 4. Storage Performance (LMDB) ---")
    
    db_path = "demo_db_storage"
    if Path(db_path).exists():
        shutil.rmtree(db_path)
    
    config = StorageConfig(path=db_path)
    storage = LMDBStorage(config)
    
    count = 10000
    logger.info(f"Writing {count} entries to LMDB...")
    
    def run_writes():
        with storage.write() as txn:
            for i in range(count):
                storage.put(txn, f"key{i}".encode(), f"value{i}".encode())
                
    measure_time("Write Latency", run_writes)
    
    def run_reads():
        with storage.read() as txn:
            for i in range(count):
                _ = storage.get(txn, f"key{i}".encode())
                
    measure_time("Read Latency", run_reads)
    
    logger.info(f"Storage Stats: {storage.stats}")
    storage.close()
    if Path(db_path).exists():
        shutil.rmtree(db_path)


async def main():
    logger.info("Starting Enterprise Performance Demo...")
    
    demo_crypto_performance()
    await demo_validation_performance()
    demo_serialization()
    demo_storage()
    
    logger.info("\nDemo Complete!")


if __name__ == "__main__":
    asyncio.run(main())
